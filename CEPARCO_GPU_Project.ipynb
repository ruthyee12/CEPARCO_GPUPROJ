{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cuda Programming Project\n",
        "\n",
        "CEPARCO S11 Group 5 Members:\n",
        "\n",
        "* Lance Victor Del Rosario\n",
        "* Audrin Matthew Javier\n",
        "* Theoni Anne Lim\n",
        "* Ruth Yee"
      ],
      "metadata": {
        "id": "ge1Lzei6Dgbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Specifications (1D Convolution):**\n",
        "\n",
        "Input: Two vectors: input vector in and output vector out.\n",
        "\n",
        "Process: 1D convolution is defined as out[i] = (in[i] + in[i+1] + in[i+2] / 3.0f\n",
        "\n",
        "Output #1: First and last 20 elements of vector out.\n",
        "\n",
        "Output #2: Video recording of the assigned CUDA concept (upload on Youtube; Should be “unlisted” and NOT “YouTube Kids”). Link to be placed in Github.\n",
        "\n",
        "\n",
        "Notes:\n",
        "* Write the kernel using the specified method in (1) C program; (2) CUDA C program using Colab platform. Place your group number and group members in the first cell.\n",
        "* CUDA program should use Unified memory, pre-fetching and memadvise.\n",
        "* Time the kernel portion only with vector size of 228 floating point.\n",
        "* For each kernel, execute at least 30 times and get the average execution time.\n",
        "* For the data, initialize each vector with values of your choice. Please document this value.\n",
        "* Check the correctness of your output. Thus, if the C version is your \"sanity check answer key,\" then the output of the CUDA version must be checked with the C version and output correspondingly (i.e., CUDA\n",
        "kernel output is correct)."
      ],
      "metadata": {
        "id": "z3EoQCaKES9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Profiler"
      ],
      "metadata": {
        "id": "z-GRVmkDS6LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ],
      "metadata": {
        "id": "soPPB0ohS5oU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cca3038-f833-4159-8a6c-6fbab8f678b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-20 15:32:20--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.46.228.170, 23.46.228.167, 23.46.228.176\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.46.228.170|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 317705436 (303M) [application/x-deb]\n",
            "Saving to: ‘nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb.1’\n",
            "\n",
            "nsight-systems-2023 100%[===================>] 302.99M  53.3MB/s    in 12s     \n",
            "\n",
            "2025-02-20 15:32:33 (24.7 MB/s) - ‘nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb.1’ saved [317705436/317705436]\n",
            "\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'nsight-systems-2023.2.3' instead of './nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb'\n",
            "nsight-systems-2023.2.3 is already the newest version (2023.2.3.1001-32894139v0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (1) C Program Version\n"
      ],
      "metadata": {
        "id": "fsD-S12WHy65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile C_1dconvolution.c\n",
        "\n",
        "// out[i] = (in[i] + in[i+1] + in[i+2]) / 3.0f\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "// ***C function version\n",
        "void oned_convolution(size_t n, float* out, float *in)\n",
        "{\n",
        "    for (size_t i = 0; i < n - 2; i++)\n",
        "        out[i] = (in[i] + in[i+1] + in[i+2]) / 3.0f;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    const size_t ARRAY_SIZE = 1 << 28;\n",
        "    const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "    // number of times the program is to be executed\n",
        "    const size_t loope = 30;\n",
        "\n",
        "    // declare arrays\n",
        "    float *in, *out;\n",
        "    in = (float*)malloc(ARRAY_BYTES);\n",
        "    out = (float*)malloc(ARRAY_BYTES);\n",
        "\n",
        "    // timer variables\n",
        "    clock_t start, end;\n",
        "\n",
        "    // initialize array\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; i++)\n",
        "        in[i] = 3.0;\n",
        "\n",
        "    // fill-in cache\n",
        "    oned_convolution(ARRAY_SIZE, out, in);\n",
        "\n",
        "    // time here\n",
        "    double elapse = 0.0, time_taken;\n",
        "    for (size_t i = 0; i < loope; i++) {\n",
        "        start = clock();\n",
        "        oned_convolution(ARRAY_SIZE, out, in);\n",
        "        end = clock();\n",
        "        time_taken = ((double)(end - start)) * 1E3 / CLOCKS_PER_SEC;\n",
        "        elapse += time_taken;\n",
        "    }\n",
        "\n",
        "    printf(\"Function (in C) average time for %lu loops is %f milliseconds to execute an array size %lu \\n\",\n",
        "           loope, elapse / loope, ARRAY_SIZE);\n",
        "\n",
        "    // error checking routine\n",
        "    size_t err_count = 0;\n",
        "    for (size_t i = 0; i < ARRAY_SIZE-2; i++) {\n",
        "        if ((in[i] + in[i+1] + in[i+2])/3.0 != out[i])\n",
        "            err_count++;\n",
        "    }\n",
        "\n",
        "    printf(\"Error count (C program): %lu\\n\", err_count);\n",
        "\n",
        "\n",
        "    // Print first and last 20 elements\n",
        "    printf(\"First 20 elements: \\n\");\n",
        "    for (size_t i = 0; i < 20; i++) {\n",
        "        printf(\"%.2f \", out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    printf(\"Last 20 elements: \\n\");\n",
        "    for (size_t i = ARRAY_SIZE - 22; i < ARRAY_SIZE; i++) {\n",
        "        printf(\"%.2f \", out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "\n",
        "    // Free memory\n",
        "    free(in);\n",
        "    free(out);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tW5gmWOH_T5",
        "outputId": "c97da516-5401-454f-d6a5-7952e6de6e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting C_1dconvolution.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "gcc C_1dconvolution.c -o C_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBi89oH6LPRC",
        "outputId": "ce2a4fc8-c2df-41db-8707-7adebf478f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./C_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvCPXbK1LXGM",
        "outputId": "7bc4971d-d34e-4445-c8d6-34c227b71583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function (in C) average time for 30 loops is 1002.400533 milliseconds to execute an array size 268435456 \n",
            "Error count (C program): 0\n",
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) CUDA Program Version"
      ],
      "metadata": {
        "id": "I6vQeZZrNRK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_1dconvolution.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA 1D convolution kernel\n",
        "__global__\n",
        "void oned_convolution(size_t n, float* out, float* in) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    for (int i = index; i < n - 2; i += stride) {\n",
        "        out[i] = (in[i] + in[i+1] + in[i+2]) / 3.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const size_t ARRAY_SIZE = 1 << 28;\n",
        "    const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "    const size_t loope = 30;\n",
        "\n",
        "    float *in, *out;\n",
        "    cudaMallocManaged(&in, ARRAY_BYTES);\n",
        "    cudaMallocManaged(&out, ARRAY_BYTES);\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    cudaGetDevice(&device);\n",
        "\n",
        "    // Memory advice\n",
        "    cudaMemAdvise(in, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "    cudaMemAdvise(in, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "    // Prefetch data to CPU\n",
        "    cudaMemPrefetchAsync(in, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "    // Prefetch data to GPU\n",
        "    cudaMemPrefetchAsync(out, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "    // Initialize input array\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; i++)\n",
        "        in[i] = 3.0;\n",
        "\n",
        "    // Prefetch data from CPU to GPU\n",
        "    cudaMemPrefetchAsync(in, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "    // Set up CUDA kernel\n",
        "    size_t numThreads = 1024;\n",
        "    size_t numBlocks = (ARRAY_SIZE - 2 + numThreads - 1) / numThreads;\n",
        "\n",
        "    printf(\"*** Function: \\n\");\n",
        "    printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
        "    printf(\"numBlocks = %lu, numThreads = %lu\\n\", numBlocks, numThreads);\n",
        "\n",
        "    for (size_t i = 0; i < loope; i++) {\n",
        "        oned_convolution<<<numBlocks, numThreads>>>(ARRAY_SIZE, out, in);\n",
        "    }\n",
        "\n",
        "    // Barrier\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Prefetch data from GPU to CPU\n",
        "    cudaMemPrefetchAsync(out, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "    // cudaMemPrefetchAsync(in, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "    // Print first and last 20 elements\n",
        "    printf(\"First 20 elements: \\n\");\n",
        "    for (size_t i = 0; i < 20; i++) {\n",
        "        printf(\"%.2f \", out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    printf(\"Last 20 elements: \\n\");\n",
        "    for (size_t i = ARRAY_SIZE - 22; i < ARRAY_SIZE; i++) {\n",
        "        printf(\"%.2f \", out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // Error check\n",
        "    size_t err_count = 0;\n",
        "    for (size_t i = 0; i < ARRAY_SIZE-2; i++) {\n",
        "        if (abs((in[i] + in[i+1] + in[i+2]) / 3.0f - out[i]) > 0.1) {\n",
        "            err_count++;\n",
        "        }\n",
        "    }\n",
        "    printf(\"Error count (CUDA program): %zu\\n\", err_count);\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(in);\n",
        "    cudaFree(out);\n",
        "\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "hFV2-prONUzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c324016-fd43-4040-c73f-501ca9a5604a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CUDA_1dconvolution.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -arch=sm_75 CUDA_1dconvolution.cu -o CUDA_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYRIFEM_X4gC",
        "outputId": "8bbca5d7-be12-4194-b011-1fdcf3ee16c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./CUDA_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxWbU7m2X8Qu",
        "outputId": "7289098e-bb1d-46ac-e7ae-57e508d0e308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==5748== NVPROF is profiling process 5748, command: ./CUDA_1dconvolution\n",
            "*** Function: \n",
            "numElements = 268435456\n",
            "numBlocks = 262144, numThreads = 1024\n",
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n",
            "Error count (CUDA program): 0\n",
            "==5748== Profiling application: ./CUDA_1dconvolution\n",
            "==5748== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  335.86ms        30  11.195ms  8.7884ms  14.792ms  oned_convolution(unsigned long, float*, float*)\n",
            "      API calls:   38.99%  425.87ms         4  106.47ms  4.1342ms  239.11ms  cudaMemPrefetchAsync\n",
            "                   30.74%  335.78ms         1  335.78ms  335.78ms  335.78ms  cudaDeviceSynchronize\n",
            "                   18.52%  202.25ms         2  101.13ms  75.403us  202.18ms  cudaMallocManaged\n",
            "                   11.60%  126.75ms         2  63.377ms  55.473ms  71.281ms  cudaFree\n",
            "                    0.11%  1.1854ms         2  592.73us  6.3770us  1.1791ms  cudaMemAdvise\n",
            "                    0.03%  335.09us        30  11.169us  3.1040us  223.30us  cudaLaunchKernel\n",
            "                    0.01%  136.49us       114  1.1970us     106ns  56.919us  cuDeviceGetAttribute\n",
            "                    0.00%  22.589us         1  22.589us  22.589us  22.589us  cudaGetDevice\n",
            "                    0.00%  12.351us         1  12.351us  12.351us  12.351us  cuDeviceGetName\n",
            "                    0.00%  7.6850us         1  7.6850us  7.6850us  7.6850us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.5030us         3     501ns     123ns  1.1950us  cuDeviceGetCount\n",
            "                    0.00%     734ns         2     367ns     127ns     607ns  cuDeviceGet\n",
            "                    0.00%     397ns         1     397ns     397ns     397ns  cuDeviceTotalMem\n",
            "                    0.00%     385ns         1     385ns     385ns     385ns  cuModuleGetLoadingMode\n",
            "                    0.00%     278ns         1     278ns     278ns     278ns  cuDeviceGetUuid\n",
            "\n",
            "==5748== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     512  2.0000MB  2.0000MB  2.0000MB  1.000000GB  89.09847ms  Host To Device\n",
            "     512  2.0000MB  2.0000MB  2.0000MB  1.000000GB  79.91190ms  Device To Host\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nsys profile ./CUDA_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNQq3nHgkREL",
        "outputId": "7eabfbf7-716b-4b74-b9ba-7e58a62684f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Function: \n",
            "numElements = 268435456\n",
            "numBlocks = 262144, numThreads = 1024\n",
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n",
            "Error count (CUDA program): 0\n",
            "Generating '/tmp/nsys-report-8041.qdstrm'\n",
            "[1/1] [========================100%] report3.nsys-rep\n",
            "Generated:\n",
            "    /content/report3.nsys-rep\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (3) CUDA with Streams and Prefetching, Mem Advise, Unified Memory"
      ],
      "metadata": {
        "id": "EkgKxRVkkmpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDAStream_1dconvolution.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define NUM_STREAMS 4  // Number of CUDA streams\n",
        "#define OVERLAP 2      // Extra elements to ensure correct convolution at boundaries\n",
        "\n",
        "// CUDA 1D convolution kernel\n",
        "__global__\n",
        "void oned_convolution(size_t n, float* out, float* in) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    for (int i = index; i < n - 2; i += stride) {\n",
        "        out[i] = (in[i] + in[i+1] + in[i+2]) / 3.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const size_t ARRAY_SIZE = 1 << 28;  // 16M elements\n",
        "    const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "    const size_t loope = 30;\n",
        "\n",
        "    float *in, *out;\n",
        "    cudaMallocManaged(&in, ARRAY_BYTES);\n",
        "    cudaMallocManaged(&out, ARRAY_BYTES);\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    cudaGetDevice(&device);\n",
        "\n",
        "    // Mem advise\n",
        "    cudaMemAdvise(in, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device);\n",
        "    cudaMemAdvise(in, ARRAY_BYTES, cudaMemAdviseSetReadMostly, device);\n",
        "\n",
        "    //prefetch data to CPU page memory\n",
        "    cudaMemPrefetchAsync(in, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "    //prefetch data to GPU page memory\n",
        "    cudaMemPrefetchAsync(out, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "    // Initialize input array\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; i++)\n",
        "        in[i] = 3.0f;\n",
        "\n",
        "    // Prefetch input data to GPU\n",
        "    cudaMemPrefetchAsync(in, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "    // Set up CUDA kernel\n",
        "    size_t numThreads = 1024;\n",
        "    size_t numBlocks = (ARRAY_SIZE - 2 + numThreads - 1) / numThreads;\n",
        "    size_t segmentSize = ARRAY_SIZE / NUM_STREAMS;\n",
        "\n",
        "    printf(\"*** Function: \\n\");\n",
        "    printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
        "    printf(\"numBlocks = %lu, numThreads = %lu\\n\", numBlocks, numThreads);\n",
        "    printf(\"Using %d CUDA Streams\\n\", NUM_STREAMS);\n",
        "\n",
        "    // Create CUDA streams\n",
        "    cudaStream_t streams[NUM_STREAMS];\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaStreamCreate(&streams[i]);\n",
        "    }\n",
        "\n",
        "    for (size_t i = 0; i < loope; i++) {\n",
        "      for (int s = 0; s < NUM_STREAMS; s++) {\n",
        "          size_t startIdx = s * segmentSize;\n",
        "          size_t endIdx = startIdx + segmentSize;\n",
        "\n",
        "          // Ensure overlap handling\n",
        "          if (s > 0) startIdx -= OVERLAP;\n",
        "          if (s < NUM_STREAMS - 1) endIdx += OVERLAP;\n",
        "\n",
        "          size_t segmentElements = endIdx - startIdx;\n",
        "          size_t segmentBytes = segmentElements * sizeof(float);\n",
        "\n",
        "          // Launch kernel in different streams\n",
        "          oned_convolution<<<(segmentElements - 2 + numThreads - 1) / numThreads, numThreads, 0, streams[s]>>>(\n",
        "              segmentElements, out + startIdx, in + startIdx);\n",
        "\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Synchronize all streams\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaStreamSynchronize(streams[i]);\n",
        "    }\n",
        "\n",
        "    // Prefetch data from GPU to CPU\n",
        "    cudaMemPrefetchAsync(in, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "    cudaMemPrefetchAsync(out, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "    // Print first and last 20 elements\n",
        "    printf(\"First 20 elements: \\n\");\n",
        "    for (size_t i = 0; i < 20; i++) {\n",
        "        printf(\"%.2f \", out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    printf(\"Last 20 elements: \\n\");\n",
        "    for (size_t i = ARRAY_SIZE - 22; i < ARRAY_SIZE; i++) {\n",
        "        printf(\"%.2f \", out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // Error check\n",
        "    size_t err_count = 0;\n",
        "    for (size_t i = OVERLAP; i < ARRAY_SIZE - OVERLAP - 2; i++) {  // Ignore overlap edges\n",
        "        float expected = (in[i] + in[i+1] + in[i+2]) / 3.0f;\n",
        "        if (abs(expected - out[i]) > 0.1) {\n",
        "            err_count++;\n",
        "        }\n",
        "    }\n",
        "    printf(\"Error count (CUDA program): %zu\\n\", err_count);\n",
        "\n",
        "    // Free memory\n",
        "    for(int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaStreamDestroy(streams[i]);\n",
        "    }\n",
        "    cudaFree(in);\n",
        "    cudaFree(out);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwn_Z9FzKMgG",
        "outputId": "ac15d83a-3d25-45a0-a3e2-602d6372449b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CUDAStream_1dconvolution.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -arch=sm_75 CUDAStream_1dconvolution.cu -o CUDAStream_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Weac7IKzMp",
        "outputId": "07ad1a4d-5a02-41b1-b8ff-47cb3a61ae8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mCUDAStream_1dconvolution.cu(74)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"segmentBytes\"\u001b[0m was declared but never referenced\n",
            "            size_t segmentBytes = segmentElements * sizeof(float);\n",
            "                   ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./CUDAStream_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG1UPb8tK3j0",
        "outputId": "f7560001-d8ce-4d2a-adad-589a53b931bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==7331== NVPROF is profiling process 7331, command: ./CUDAStream_1dconvolution\n",
            "*** Function: \n",
            "numElements = 268435456\n",
            "numBlocks = 262144, numThreads = 1024\n",
            "Using 4 CUDA Streams\n",
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n",
            "Error count (CUDA program): 0\n",
            "==7331== Profiling application: ./CUDAStream_1dconvolution\n",
            "==7331== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  317.57ms       120  2.6464ms  2.2024ms  3.7114ms  oned_convolution(unsigned long, float*, float*)\n",
            "      API calls:   41.19%  429.11ms         5  85.822ms  733.46us  239.08ms  cudaMemPrefetchAsync\n",
            "                   30.41%  316.83ms         4  79.208ms  2.4059ms  309.57ms  cudaStreamSynchronize\n",
            "                   21.87%  227.87ms         2  113.94ms  76.041us  227.80ms  cudaMallocManaged\n",
            "                    6.42%  66.850ms         2  33.425ms  32.177ms  34.673ms  cudaFree\n",
            "                    0.07%  692.21us       120  5.7680us  3.2630us  206.55us  cudaLaunchKernel\n",
            "                    0.02%  181.65us       114  1.5930us     121ns  64.293us  cuDeviceGetAttribute\n",
            "                    0.01%  78.688us         4  19.672us  3.1800us  57.315us  cudaStreamCreate\n",
            "                    0.01%  57.056us         4  14.264us  4.2220us  42.668us  cudaStreamDestroy\n",
            "                    0.00%  26.271us         1  26.271us  26.271us  26.271us  cuDeviceGetName\n",
            "                    0.00%  17.856us         2  8.9280us  2.4810us  15.375us  cudaMemAdvise\n",
            "                    0.00%  6.1110us         1  6.1110us  6.1110us  6.1110us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.9630us         3     987ns     196ns  2.3920us  cuDeviceGetCount\n",
            "                    0.00%  1.7970us         1  1.7970us  1.7970us  1.7970us  cudaGetDevice\n",
            "                    0.00%  1.3550us         2     677ns     242ns  1.1130us  cuDeviceGet\n",
            "                    0.00%     781ns         1     781ns     781ns     781ns  cuModuleGetLoadingMode\n",
            "                    0.00%     735ns         1     735ns     735ns     735ns  cuDeviceTotalMem\n",
            "                    0.00%     333ns         1     333ns     333ns     333ns  cuDeviceGetUuid\n",
            "\n",
            "==7331== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     512  2.0000MB  2.0000MB  2.0000MB  1.000000GB  88.96109ms  Host To Device\n",
            "     512  2.0000MB  2.0000MB  2.0000MB  1.000000GB  79.96757ms  Device To Host\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this to generate a downloadable file to view in the NSight Profiler"
      ],
      "metadata": {
        "id": "2j3AV6yKTEC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nsys profile ./CUDAStream_1dconvolution"
      ],
      "metadata": {
        "id": "dofkdfbYS8yJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1839cb85-ce61-4e66-c3b7-6a60c3c8c53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Function: \n",
            "numElements = 16777216\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Using 4 CUDA Streams\n",
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n",
            "Error count (CUDA program): 0\n",
            "Generating '/tmp/nsys-report-afd3.qdstrm'\n",
            "[1/1] [========================100%] report4.nsys-rep\n",
            "Generated:\n",
            "    /content/report4.nsys-rep\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (4) CUDA with Streams and Memcpy (no prefetching, mem advise, or unified memory)"
      ],
      "metadata": {
        "id": "_sZC6vvCLmtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDAstreamonly_1dconvolution.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define NUM_STREAMS 4\n",
        "#define OVERLAP 2\n",
        "__global__\n",
        "void oned_convolution(size_t n, float* out, float* in) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < n - 2)\n",
        "        out[i] = (in[i] + in[i+1] + in[i+2]) / 3.0f;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    const size_t ARRAY_SIZE = 1 << 28;\n",
        "    const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "    const size_t loope = 30;\n",
        "\n",
        "    //initialize and allocate\n",
        "    float *host_in, *host_out;\n",
        "    float *dev_in, *dev_out;\n",
        "    //host_in = (float*)malloc(ARRAY_BYTES);\n",
        "    //host_out = (float*)malloc(ARRAY_BYTES);\n",
        "\n",
        "    cudaMallocHost((void**)&host_in, ARRAY_BYTES);\n",
        "    cudaMallocHost((void**)&host_out, ARRAY_BYTES);\n",
        "\n",
        "    //initialize values\n",
        "    for(size_t i = 0; i < ARRAY_SIZE; i++){\n",
        "      host_in[i] = 3.0;\n",
        "    }\n",
        "\n",
        "    cudaMalloc((void**)&dev_in, ARRAY_BYTES);\n",
        "    cudaMalloc((void**)&dev_out, ARRAY_BYTES);\n",
        "\n",
        "    //stream creation and running\n",
        "    cudaStream_t streams[NUM_STREAMS];\n",
        "    for(int i = 0; i < NUM_STREAMS; i++){\n",
        "      cudaStreamCreate(&streams[i]);\n",
        "    }\n",
        "\n",
        "\n",
        "    //event creation\n",
        "    cudaEvent_t event_start[NUM_STREAMS], event_end[NUM_STREAMS];\n",
        "    for(int i = 0; i < NUM_STREAMS; i++){\n",
        "      cudaEventCreate(&event_start[i]);\n",
        "      cudaEventCreate(&event_end[i]);\n",
        "    }\n",
        "\n",
        "    size_t numThreads = 1024;\n",
        "    //size_t numBlocks = (ARRAY_SIZE - 2 + numThreads - 1) / numThreads;\n",
        "    size_t segmentSize = ARRAY_SIZE / NUM_STREAMS;\n",
        "    //size_t segmentBytes = segmentSize * sizeof(float);\n",
        "\n",
        "    //Host to Device\n",
        "    for(int i = 0; i < NUM_STREAMS; i++) {\n",
        "      size_t startIdx = i * segmentSize;\n",
        "      size_t endIdx = startIdx + segmentSize;\n",
        "\n",
        "      if (i > 0) startIdx -= OVERLAP;\n",
        "      if (i < NUM_STREAMS - 1) endIdx += OVERLAP;\n",
        "\n",
        "      size_t segmentElements = endIdx - startIdx;\n",
        "      size_t segmentBytes = segmentElements * sizeof(float);\n",
        "\n",
        "      cudaMemcpyAsync(dev_in + startIdx, host_in + startIdx, segmentBytes, cudaMemcpyHostToDevice, streams[i]);\n",
        "\n",
        "      cudaEventRecord(event_start[i], streams[i]);\n",
        "    }\n",
        "\n",
        "    //cudaDeviceSynchronize();\n",
        "\n",
        "    //actual kernel run on multiple streams\n",
        "    //for(size_t i = 0; i < loope; i++){\n",
        "      for(int s = 0; s < NUM_STREAMS; s++){\n",
        "        size_t startIdx = s * segmentSize;\n",
        "          size_t endIdx = startIdx + segmentSize;\n",
        "\n",
        "          // Ensure overlap handling\n",
        "          if (s > 0) startIdx -= OVERLAP;\n",
        "          if (s < NUM_STREAMS - 1) endIdx += OVERLAP;\n",
        "\n",
        "          size_t segmentElements = endIdx - startIdx;\n",
        "          //size_t segmentBytes = segmentElements * sizeof(float);\n",
        "\n",
        "          cudaStreamWaitEvent(streams[s], event_start[s], 0);\n",
        "\n",
        "          // Launch kernel in different streams\n",
        "            oned_convolution<<<(segmentElements - 2 + numThreads - 1) / numThreads, numThreads, 0, streams[s]>>>(segmentElements, dev_out + startIdx, dev_in + startIdx);\n",
        "\n",
        "            cudaEventRecord(event_end[s], streams[s]);\n",
        "      }\n",
        "    //}\n",
        "\n",
        "    // Device to Host\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        size_t startIdx = i * segmentSize;\n",
        "        size_t endIdx = startIdx + segmentSize;\n",
        "\n",
        "        if (i > 0) startIdx -= OVERLAP;\n",
        "        if (i < NUM_STREAMS - 1) endIdx += OVERLAP;\n",
        "\n",
        "        size_t segmentElements = endIdx - startIdx;\n",
        "        size_t segmentBytes = segmentElements * sizeof(float);\n",
        "\n",
        "        cudaStreamWaitEvent(streams[i], event_end[i], 0);\n",
        "        cudaMemcpyAsync(host_out + startIdx, dev_out + startIdx, segmentBytes, cudaMemcpyDeviceToHost, streams[i]);\n",
        "    }\n",
        "\n",
        "    //synching\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "\n",
        "    //print first and last 20 elements\n",
        "    printf(\"First 20 elements: \\n\");\n",
        "    for (size_t i = 0; i < 20; i++) {\n",
        "        printf(\"%.2f \", host_out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    printf(\"Last 20 elements: \\n\");\n",
        "    for (size_t i = ARRAY_SIZE - 22; i < ARRAY_SIZE; i++) {\n",
        "        printf(\"%.2f \", host_out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // Error check\n",
        "    size_t err_count = 0;\n",
        "    for (size_t i = 0; i < ARRAY_SIZE-2; i++) {\n",
        "        if (abs((host_in[i] + host_in[i+1] + host_in[i+2]) / 3.0f - host_out[i]) > 0.1) {\n",
        "            err_count++;\n",
        "        }\n",
        "    }\n",
        "    printf(\"Error count (CUDA program): %zu\\n\", err_count);\n",
        "\n",
        "    //cleanup\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaStreamDestroy(streams[i]);\n",
        "        cudaEventDestroy(event_start[i]);\n",
        "        cudaEventDestroy(event_end[i]);\n",
        "    }\n",
        "    cudaFree(dev_in);\n",
        "    cudaFree(dev_out);\n",
        "    cudaFreeHost(host_in);\n",
        "    cudaFreeHost(host_out);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odRSYHaxLzxS",
        "outputId": "c6bfa52f-7f32-4fd8-fe07-0cb606b265e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CUDAstreamonly_1dconvolution.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -arch=sm_75 CUDAstreamonly_1dconvolution.cu -o CUDAstreamonly_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipiCJffDQ0rc",
        "outputId": "5a5ced7f-8184-4136-cf43-f190d3e9e649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mCUDAstreamonly_1dconvolution.cu(19)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"loope\"\u001b[0m was declared but never referenced\n",
            "      const size_t loope = 30;\n",
            "                   ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./CUDAstreamonly_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0iNHT3-Q5JO",
        "outputId": "8d2b9d08-0f4f-4736-ef09-dbc3a61cd42c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==6957== NVPROF is profiling process 6957, command: ./CUDAstreamonly_1dconvolution\n",
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n",
            "Error count (CUDA program): 0\n",
            "==6957== Profiling application: ./CUDAstreamonly_1dconvolution\n",
            "==6957== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   49.57%  92.833ms         4  23.208ms  21.932ms  26.138ms  [CUDA memcpy HtoD]\n",
            "                   42.73%  80.027ms         4  20.007ms  19.922ms  20.172ms  [CUDA memcpy DtoH]\n",
            "                    7.70%  14.429ms         4  3.6072ms  3.6012ms  3.6214ms  oned_convolution(unsigned long, float*, float*)\n",
            "      API calls:   60.96%  1.21269s         2  606.35ms  499.03ms  713.66ms  cudaMallocHost\n",
            "                   29.91%  594.95ms         2  297.48ms  293.94ms  301.01ms  cudaFreeHost\n",
            "                    4.67%  92.944ms         4  23.236ms  6.3270us  92.922ms  cudaLaunchKernel\n",
            "                    4.20%  83.605ms         1  83.605ms  83.605ms  83.605ms  cudaDeviceSynchronize\n",
            "                    0.20%  3.9912ms         2  1.9956ms  1.0752ms  2.9159ms  cudaFree\n",
            "                    0.03%  524.82us         2  262.41us  153.53us  371.29us  cudaMalloc\n",
            "                    0.01%  201.57us       114  1.7680us     138ns  80.935us  cuDeviceGetAttribute\n",
            "                    0.00%  82.395us         8  10.299us  3.7950us  33.179us  cudaMemcpyAsync\n",
            "                    0.00%  64.375us         4  16.093us  4.8320us  49.123us  cudaStreamDestroy\n",
            "                    0.00%  50.653us         4  12.663us  2.2540us  43.739us  cudaStreamCreate\n",
            "                    0.00%  23.238us         8  2.9040us  1.3060us  8.1240us  cudaEventRecord\n",
            "                    0.00%  13.813us         1  13.813us  13.813us  13.813us  cuDeviceGetName\n",
            "                    0.00%  12.438us         8  1.5540us     426ns  9.0530us  cudaEventCreate\n",
            "                    0.00%  11.786us         8  1.4730us     643ns  3.6200us  cudaStreamWaitEvent\n",
            "                    0.00%  11.207us         8  1.4000us     721ns  4.3100us  cudaEventDestroy\n",
            "                    0.00%  5.8460us         1  5.8460us  5.8460us  5.8460us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8320us         3     610ns     162ns  1.3480us  cuDeviceGetCount\n",
            "                    0.00%  1.1430us         2     571ns     221ns     922ns  cuDeviceGet\n",
            "                    0.00%     582ns         1     582ns     582ns     582ns  cuDeviceTotalMem\n",
            "                    0.00%     531ns         1     531ns     531ns     531ns  cuDeviceGetUuid\n",
            "                    0.00%     416ns         1     416ns     416ns     416ns  cuModuleGetLoadingMode\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nsys profile ./CUDAstreamonly_1dconvolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G80Qb8J4bf1Y",
        "outputId": "c47f6a61-5999-4e97-c101-ebc11d0b371b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n",
            "Error count (CUDA program): 0\n",
            "Generating '/tmp/nsys-report-aa19.qdstrm'\n",
            "[1/1] [========================100%] report5.nsys-rep\n",
            "Generated:\n",
            "    /content/report5.nsys-rep\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (5) CUDA with Streams and Memcpy for each loope (no prefetching, mem advise, or unified memory)\n",
        "\n"
      ],
      "metadata": {
        "id": "bkWltjPlBLlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define NUM_STREAMS 4\n",
        "#define OVERLAP 2\n",
        "\n",
        "__global__\n",
        "void oned_convolution(size_t n, float* out, float* in) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < n - 2)\n",
        "        out[i] = (in[i] + in[i+1] + in[i+2]) / 3.0f;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const size_t ARRAY_SIZE = 1 << 28; // 26M elements\n",
        "    const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "    const size_t loope = 30;\n",
        "\n",
        "    // Initialize and allocate\n",
        "    float *host_in, *host_out;\n",
        "    float *dev_in, *dev_out;\n",
        "\n",
        "    // Use pinned memory for better transfer performance\n",
        "    cudaMallocHost((void**)&host_in, ARRAY_BYTES);\n",
        "    cudaMallocHost((void**)&host_out, ARRAY_BYTES);\n",
        "\n",
        "    // Initialize input values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; i++) {\n",
        "        host_in[i] = 3.0;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&dev_in, ARRAY_BYTES);\n",
        "    cudaMalloc((void**)&dev_out, ARRAY_BYTES);\n",
        "\n",
        "    // Create streams and events\n",
        "    cudaStream_t streams[NUM_STREAMS];\n",
        "    cudaEvent_t event_start[NUM_STREAMS], event_end[NUM_STREAMS];\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaStreamCreate(&streams[i]);\n",
        "        cudaEventCreate(&event_start[i]);\n",
        "        cudaEventCreate(&event_end[i]);\n",
        "    }\n",
        "\n",
        "    size_t numThreads = 1024;\n",
        "    size_t segmentSize = ARRAY_SIZE / NUM_STREAMS;\n",
        "\n",
        "    // Main loop for overlapping memory transfers and kernel execution\n",
        "    for (size_t i = 0; i < loope + 2; i++) {\n",
        "        for (int s = 0; s < NUM_STREAMS; s++) {\n",
        "            size_t startIdx = s * segmentSize;\n",
        "            size_t endIdx = startIdx + segmentSize;\n",
        "\n",
        "            // Adjust for overlap\n",
        "            if (s > 0) startIdx -= OVERLAP;\n",
        "            if (s < NUM_STREAMS - 1) endIdx += OVERLAP;\n",
        "\n",
        "            size_t segmentElements = endIdx - startIdx;\n",
        "            size_t segmentBytes = segmentElements * sizeof(float);\n",
        "\n",
        "            // Copy data to device\n",
        "            cudaMemcpyAsync(dev_in + startIdx, host_in + startIdx, segmentBytes, cudaMemcpyHostToDevice, streams[s]);\n",
        "\n",
        "            // Record event after memory transfer\n",
        "            cudaEventRecord(event_start[s], streams[s]);\n",
        "\n",
        "            // Wait for memory transfer to complete before launching kernel\n",
        "            cudaStreamWaitEvent(streams[s], event_start[s], 0);\n",
        "\n",
        "            // Launch kernel\n",
        "            oned_convolution<<<(segmentElements - 2 + numThreads - 1) / numThreads, numThreads, 0, streams[s]>>>(segmentElements, dev_out + startIdx, dev_in + startIdx);\n",
        "\n",
        "            // Record event after kernel execution\n",
        "            cudaEventRecord(event_end[s], streams[s]);\n",
        "\n",
        "            // Wait for kernel to complete before copying data back\n",
        "            cudaStreamWaitEvent(streams[s], event_end[s], 0);\n",
        "\n",
        "            // Copy data back to host\n",
        "            cudaMemcpyAsync(host_out + startIdx, dev_out + startIdx, segmentBytes, cudaMemcpyDeviceToHost, streams[s]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Synchronize all streams at the end\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Print first and last 20 elements\n",
        "    printf(\"First 20 elements: \\n\");\n",
        "    for (size_t i = 0; i < 20; i++) {\n",
        "        printf(\"%.2f \", host_out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    printf(\"Last 20 elements: \\n\");\n",
        "    for (size_t i = ARRAY_SIZE - 22; i < ARRAY_SIZE; i++) {\n",
        "        printf(\"%.2f \", host_out[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // Error check\n",
        "    size_t err_count = 0;\n",
        "    for (size_t i = 0; i < ARRAY_SIZE - 2; i++) {\n",
        "        if (abs((host_in[i] + host_in[i+1] + host_in[i+2]) / 3.0f - host_out[i]) > 0.1) {\n",
        "            err_count++;\n",
        "        }\n",
        "    }\n",
        "    printf(\"Error count (CUDA program): %zu\\n\", err_count);\n",
        "\n",
        "    // Cleanup\n",
        "    for (int i = 0; i < NUM_STREAMS; i++) {\n",
        "        cudaStreamDestroy(streams[i]);\n",
        "        cudaEventDestroy(event_start[i]);\n",
        "        cudaEventDestroy(event_end[i]);\n",
        "    }\n",
        "    cudaFree(dev_in);\n",
        "    cudaFree(dev_out);\n",
        "    cudaFreeHost(host_in);\n",
        "    cudaFreeHost(host_out);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt9YX8qKlh4g",
        "outputId": "b75e9c33-c7d7-4ba6-af30-853a9892c3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -arch=sm_75 test.cu -o test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUe9v1Rslp5z",
        "outputId": "9176fda1-0925-40c3-d7d5-58be8747960c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmoVEDWolwZ8",
        "outputId": "6423f7e4-193f-4613-c848-8d6dd72a7ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==6121== NVPROF is profiling process 6121, command: ./test\n",
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n",
            "Error count (CUDA program): 0\n",
            "==6121== Profiling application: ./test\n",
            "==6121== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   47.95%  3.58320s       128  27.994ms  23.727ms  30.685ms  [CUDA memcpy HtoD]\n",
            "                   47.62%  3.55850s       128  27.801ms  19.932ms  30.851ms  [CUDA memcpy DtoH]\n",
            "                    4.42%  330.48ms       128  2.5819ms  2.3671ms  3.6282ms  oned_convolution(unsigned long, float*, float*)\n",
            "      API calls:   66.57%  3.57907s         1  3.57907s  3.57907s  3.57907s  cudaDeviceSynchronize\n",
            "                   21.86%  1.17515s         2  587.58ms  489.39ms  685.77ms  cudaMallocHost\n",
            "                   10.97%  589.68ms         2  294.84ms  287.88ms  301.79ms  cudaFreeHost\n",
            "                    0.49%  26.230ms       128  204.92us  3.6090us  25.659ms  cudaLaunchKernel\n",
            "                    0.07%  3.8772ms         2  1.9386ms  987.19us  2.8900ms  cudaFree\n",
            "                    0.01%  786.81us       256  3.0730us  2.1570us  31.116us  cudaMemcpyAsync\n",
            "                    0.01%  562.54us         2  281.27us  182.12us  380.42us  cudaMalloc\n",
            "                    0.01%  372.47us       256  1.4540us  1.1830us  6.7020us  cudaEventRecord\n",
            "                    0.00%  152.69us       256     596ns     520ns  2.6260us  cudaStreamWaitEvent\n",
            "                    0.00%  133.48us       114  1.1700us     108ns  55.390us  cuDeviceGetAttribute\n",
            "                    0.00%  71.061us         4  17.765us  2.3400us  62.956us  cudaStreamCreate\n",
            "                    0.00%  58.150us         4  14.537us  3.9470us  45.131us  cudaStreamDestroy\n",
            "                    0.00%  12.380us         1  12.380us  12.380us  12.380us  cuDeviceGetName\n",
            "                    0.00%  12.342us         8  1.5420us     464ns  6.8240us  cudaEventCreate\n",
            "                    0.00%  9.8650us         8  1.2330us     586ns  4.0130us  cudaEventDestroy\n",
            "                    0.00%  5.7970us         1  5.7970us  5.7970us  5.7970us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6250us         3     541ns     123ns  1.2880us  cuDeviceGetCount\n",
            "                    0.00%     827ns         2     413ns     127ns     700ns  cuDeviceGet\n",
            "                    0.00%     523ns         1     523ns     523ns     523ns  cuModuleGetLoadingMode\n",
            "                    0.00%     421ns         1     421ns     421ns     421ns  cuDeviceTotalMem\n",
            "                    0.00%     276ns         1     276ns     276ns     276ns  cuDeviceGetUuid\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nsys profile ./test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnodIcx8mByh",
        "outputId": "19500d11-889d-40fc-db22-05f6da20c45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "Last 20 elements: \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 0.00 0.00 \n",
            "Error count (CUDA program): 0\n",
            "Generating '/tmp/nsys-report-0150.qdstrm'\n",
            "[1/1] [========================100%] report6.nsys-rep\n",
            "Generated:\n",
            "    /content/report6.nsys-rep\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}